{"meta":{"title":"Pyr8 travels CV world","subtitle":null,"description":null,"author":"Henry Yang","url":"http://guigua1.github.io","root":"/"},"pages":[{"title":"[404]","date":"2019-12-10T11:20:55.904Z","updated":"2019-12-10T11:20:55.904Z","comments":true,"path":"404.html","permalink":"http://guigua1.github.io/404.html","excerpt":"","text":""},{"title":"Bio/CV","date":"2019-12-10T11:21:13.000Z","updated":"2019-12-10T11:26:24.385Z","comments":true,"path":"about/index.html","permalink":"http://guigua1.github.io/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Multiple Lane Boundary Detection Using A Combination of Low-Level Image Features","slug":"lane-detect/Multiple Lane Boundary Detection Using A Combination of Low-Level Image Features","date":"2019-12-10T03:10:57.096Z","updated":"2019-12-10T11:04:32.000Z","comments":true,"path":"2019/12/lane-detect/Multiple Lane Boundary Detection Using A Combination of Low-Level Image Features/","link":"","permalink":"http://guigua1.github.io/2019/12/lane-detect/Multiple%20Lane%20Boundary%20Detection%20Using%20A%20Combination%20of%20Low-Level%20Image%20Features/","excerpt":"","text":"framework Fusion of edge features and high intensity feature Application of Kalman ﬁlter for information fusion and tracking Test on roads with different driving scenarios, including day, night, heavy trafﬁc, rain, confusing textures and shadows intro Detection multiple lanes and achieving reliability under different environments are remaining challenges. At first, for vision-based feature extraction, intensity is unreliable and should be combined with other constraints, such as parallel boundaries and intersect around vanishing point. However, without a priori, performance often degrades. Therefore, edge feature and high intensity feature are fused to enhance the stability of feature measurement. Variance of measurement noise is estimated live and indicates the corresponding interference. Moreover, Kalman filter is used in case that both measurements are contaminated. 1. Edge Feature Extractiongraph LR subgraph edge edge --fitting--> line line --grouping--> boundary end subgraph intensity boundary --searching--> 1[high intensity] 1 --refining--> lane end ROI is defined below a hand-calibrated horizontal line. Canny + Hough Transform Support lines $y^1$ and $y^2$ below the horizontal line, and intersections with edge lines Edge feature: $\\mathbf{I}_i^e = [ x_i^1, x_i^2]^T \\in \\mathbb{R}^2$ Grouping: Sort $\\mathbf{I}_i^e$ by slope $1 / (x_i^1 - x_i^2)$ Compute distance $\\Delta\\mathbf{d}{i,i+1} = [x_i^1-\\{i+1}^2|, |xi^2-x{i+1}^2|]$ Compare $\\Delta\\mathbf{d}_{ij}$ with threshold $t=[\\frac{w}{2n_1}, \\frac{w}{2n_2}]$, where $w$ is the width of the image frame, and $n_1, n_2$ is the maximum number of lanes that can be held in $w$ on $y^1, y^2$, respectively. Average lines: $\\hat{\\mathbf{I}}k^e = \\frac{1}{n_k}\\sum{l=1}^{nk}\\mathbf{I}{i+l}^e$ Searching local maxima features at intervals along boundary $d$ horizontal pixels beside the boundary are convolved with a first derivative Gaussian Kernel G'(x) = \\frac{-x}{\\sqrt{2\\pi}\\sigma^3}\\exp^\\frac{-x^2}{2\\sigma^2}also, $\\sigma$ varies from 31 for the lower half boundary to 21 for the upper half. Smoothness score of local maxima feature \\gamma_k = \\frac{1}{2N_k} \\sum_{i,j;i\\neq j}^{N_k}(\\Delta S_{ij} - \\mu_{\\Delta S_{ij}})^2 \\\\ \\Delta S_{ij} = |\\frac{x_i - x_j}{y_i - y_j}|, \\quad i,j\\in \\mathbb{Z}^+;i\\neq j RANSAC refining with $\\gamma_k$ lower than a certain value, get $\\mathbf{I}_i^m$ 2. Kalman Filterstate vector: lane intersections on the two support lines, lateral speed of boundary movement on the support lines, and interference term. 3. Performancedata: 1024x480 pixels at 30 fps Avg. Precision: 90.9% Avg. Recall: 91.0% Processing Time: 0.0130 sec per frame 5. ReviewMany priori parameters are implicit in this paper, so that it is hard to recall consistent performance for my experiments. Reference","categories":[{"name":"Article Review","slug":"Article-Review","permalink":"http://guigua1.github.io/categories/Article-Review/"}],"tags":[]},{"title":"Real-Time Lane-Vehicle Detection and Tracking System","slug":"lane-detect/Real-Time Lane-Vehicle Detection and Tracking System","date":"2019-10-05T16:00:00.000Z","updated":"2019-12-10T11:03:09.000Z","comments":true,"path":"2019/10/lane-detect/Real-Time Lane-Vehicle Detection and Tracking System/","link":"","permalink":"http://guigua1.github.io/2019/10/lane-detect/Real-Time%20Lane-Vehicle%20Detection%20and%20Tracking%20System/","excerpt":"","text":"framework lane detection and tracking sky and road region recognition vehicle detection and tracking introThis paper is inspired by the advantages of object detector based on CNN, such as Faster-RCNN, YOLO, and so on. Therefore, car detection is the core step in the framework, whereas the lane detection and tracking is used to reduce ROI regions. 1. Lane detection and trackingThis traditional pipeline is adopted: graph LR subgraph detect 1[RGB] --> 2[Gray] 2 --> 3[Gauss Blur] 3 --> 4[Canny] 4 --> 5[Hough Transform] end 5 -.-> 6 subgraph split 6[Slope filter] --> 7[left/right split] end 7 -.-> 8 subgraph track 8[change compute] --> 9[update lanes] end 2. Sky and road region recognitionRoad region: graph LR a[lane information] --> b[Canny edge] b --> c[region growing from bottom] Sky region: graph LR 1[vertical mean distribution] --> 2[threshold] 3. Vehicle detectionA simpler CNN to be more easily converged is proposed, which does not predict the coordinates of the object bounding boxes directly. Otherwise, the b-boxes are K centroids from clustering of ground truth, and DPM regression is used to adjust them. Moreover, with box confidences, the loss is weighted sum of the regression and classification. L(x,c,l,g) = BCE(x,c)+\\alpha L_2(l, g)Then, the authors declared three innovations: A general preprocessing is adopted. The loss of the regression part is improved, by assigning any boxes with a larger overlap than the threshold to the ground truth. Negative samples are added. CNN Model: Alex-Net with replaced top classification layer. Training process: graph LR 1[300 videos on highway] --> 2[5000/1000 vehicle images for train/test] multi-scale overlap &lt; 0.2 as negative | overlap &gt; 0.8 as positive Aspect ratios in range of [0.8, 1.2] are accepted, and paddings are performed 30000 epochs, batch size of 128, momentum of 0.9 weight decay 0.0005, lr 0.001/20000 and 0.0001/10000 4. Vehicle trackingImproved FB-error tracking framework based on sparse Pyramid optical flow is used2. 5. Result65% AP and 90% Recall, nothing impressive. Reference 1. Guan, Huang , et al. “Real-Time Lane-Vehicle Detection and Tracking System,” 2016 Chinese Control and Decision Conference (CCDC) IEEE, 2016. &#8617; 2. P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object detection with discriminatively trained part-based models,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 32, pp. 1627-1645, 2010. &#8617;","categories":[{"name":"Article Review","slug":"Article-Review","permalink":"http://guigua1.github.io/categories/Article-Review/"}],"tags":[]},{"title":"视觉消失点方法","slug":"lane-detect/vanishing point detection","date":"2019-10-03T16:00:00.000Z","updated":"2019-12-13T14:18:26.940Z","comments":true,"path":"2019/10/lane-detect/vanishing point detection/","link":"","permalink":"http://guigua1.github.io/2019/10/lane-detect/vanishing%20point%20detection/","excerpt":"","text":"摘要车道检测的应用环境多为开阔的室外场地，无论是在结构化环境（铺装道路）或非结构化环境（非铺装道路，off-road）中，此时的道路远端往往也位于图像的视觉消失点，因而可以将消失点作为车道检测的强有力证据。与此同时，想要得到精确地估计视觉消失点本身也具有一定难点。例如以边缘进行估计时，与消失点无关的物体边缘或环境（如光线强弱、噪声等）对边缘细节的破坏，可能给估计过程带来干扰，因此需要设计或选择具有一定辨别力及不变性的边缘特征表示方法。除此之外，各个特征对视觉消失点的贡献对于其估计的精度和效率都具有重要影响。一些文献针对这些问题展开了相关研究，下文基于几种经典的视觉消失点检测方法1234进行介绍。 1. 问题描述在基本的成像模型下，3-D世界中的平行线，如道路、建筑物边界等，透视投影在2-D图像平面中的线条将汇聚到特定位置，这些位置即称为视觉消失点。常见图像中可以观察出一个至三个消失点，而具体从车辆的前视摄像头来看，直线道路的消失点往往汇聚于视线前方，且与地平线相交。需要注意的是，曲线道路的消失点往往存在偏移。通过Moghandam P. 等人的论文4所示图片可以直观地理解视觉消失点vp (Vanishing-Point)的位置。 结合上图，大多数方法将消失点表示为2-D图像中部分直线的交点，于是相应的检测过程可大致分为三个步骤： 检测图像中的显著边界，如道路或车道线等 判断其中的有效边界，如直线且长度或方向符合所需条件的边界 通过有效边界的延展，并计算相交点或对相应位置进行投票，估计可能的视觉消失点。 其中涉及到的一些算法诸如Canny边缘检测、Hough直线检测、RANSAC对于大多数场景具有一定的适应性，但也存在显而易见的缺陷。通常来说，在非结构化的道路条件下往往不具有明显的边界特征，同时也难以构成连续的直线。因此，为了提高其他应用场景下的道路和消失点检测效果，不依赖边缘检测的方法得到了广泛研究。 (to be continue…) 参考文献 1. Rasmussen C . Grouping Dominant Orientations for Ill-Structured Road Following.[C]// IEEE Computer Society Conference on Computer Vision &amp; Pattern Recognition. IEEE, 2004. &#8617; 2. Kong H, Audibert J Y, Ponce J. Vanishing point detection for road detection[C]// 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA. IEEE, 2009. &#8617; 3:Kong H, Audibert J Y, Ponce J. General Road Detection From a Single Image[J]. IEEE Transactions on Image Processing, 2010, 19(8):2211-2220.4: Moghadam P, Starzyk J A, Wijesoma W S. Fast Vanishing-Point Detection in Unstructured Environments[J]. IEEE Transactions on Image Processing, 2012, 21(1):425-430. 5: An Efficient Lane Markings Detection and Tracking Method based on Vanishing Point Constraints","categories":[{"name":"Article Review","slug":"Article-Review","permalink":"http://guigua1.github.io/categories/Article-Review/"}],"tags":[]},{"title":"Evaluation of Road Marking Feature Extraction","slug":"lane-detect/Evaluation of Road Marking Feature Extraction","date":"2019-10-02T16:00:00.000Z","updated":"2019-12-13T14:22:02.372Z","comments":true,"path":"2019/10/lane-detect/Evaluation of Road Marking Feature Extraction/","link":"","permalink":"http://guigua1.github.io/2019/10/lane-detect/Evaluation%20of%20Road%20Marking%20Feature%20Extraction/","excerpt":"","text":"在传统车道线检测算法中，手工设计的特征是算法的关键之一。为了评估不同特征的特点和差异，《Evaluation of Road Marking Feature Extraction》这篇文章中介绍了部分较为基础的图像特征，例如从像素强度或各种滤波响应进行阈值分割得到的像素点，并基于简单的道路标记检测方法进行了对比分析。 frameworkgraph LR subgraph Evaluation 1[extracting marking features] end 1 --> 2[estimating geometrical model] 2 --> 3[tracking parameters] Feature extraction Methods: thresholding / gradient analysis / convolution 1. Intro Six representative road feature extractors and two variants based on thresholding, gradient analysis and convolution were selected. A systematical evaluation of the extraction step is proposed based on manually labeled ground truth. 2. Method ReviewExtracting features from a single image captured by a camera in front of a vehicle. graph LR 1[extractors] --> 2[geometric] 2[geometric] --> 3[segments] 2[geometric] -.- 4[combined] 1 --> 5[photometric] 5 -.- 4 4 --> 6[pairwise gradients] 4 --> 7[convolution filters] 4 --> 8[ridges] 5 --> 9[light pixels] 5 --> 10[edges] Positive-Negative gradients extract a line $y{init} \\to y{end}$ where $y{end} &gt; T_G, ~ y{init}&lt;-T_G$, when the line lies within the range $[S_m, S_M]$, and the image intensity between the line is higher than the intensity at the two endpoints. BTW, “Strong gradient” only use local gradient maxima greater than $T_g$ Steerable filters exploit the spatial continuity of lane markings Each filter is obtained as linear combination of three basis filters corresponding to the second derivatives of Gaussian distribution. Top-Hat filters with variant scales dedicate to vertical lane markings Global / local / symmetrical local threshold 3. Evaluation116 road scene images 4. ReviewThe ROC curves are very confusing, because the FPR (False Positive Rate) became an indicator for better performance in this paper.","categories":[{"name":"Article Review","slug":"Article-Review","permalink":"http://guigua1.github.io/categories/Article-Review/"}],"tags":[]},{"title":"lane detection and lane departure warning review","slug":"lane-detect/Lane detection and lane departure warning review","date":"2019-10-01T16:00:00.000Z","updated":"2019-12-10T11:04:58.000Z","comments":true,"path":"2019/10/lane-detect/Lane detection and lane departure warning review/","link":"","permalink":"http://guigua1.github.io/2019/10/lane-detect/Lane%20detection%20and%20lane%20departure%20warning%20review/","excerpt":"","text":"1.8. Lane Detection车道检测是上述步骤的综合应用，如将边界等特征提取与道路模型相结合。因此，基于不同的侧重点，存在两类方法，即基于特征和基于模型的方法。 首先，特征方法通过提取道路图像的低级特征来定位车道。除通用的边界检测算子（如Sobel / Canny）外，部分文章提出了针对车道线的专业边界检测方法： 另外一些方法对边界进一步具象，如[52]应用Hough变换提取车道轮廓的线段，并使用DBSCAN方法进行聚类分析。[53]使用最大稳定极值区域（MSER）与PPHough变换提取车道标记。[54][55]则结合车辆检测决定检测车道的区域。 [28,29,63] 边缘分布函数（EDF) [33] 视觉消失点与Hough变换结合","categories":[{"name":"Article Review","slug":"Article-Review","permalink":"http://guigua1.github.io/categories/Article-Review/"}],"tags":[]}]}